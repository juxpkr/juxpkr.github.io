---
layout: single
title: "[실습정리] Azure VM 배포 및 WordPress 설치"
date: 2025-04-09 23:00:00 +0900
category:
  - learn
tags:
  - Azure
  - SSH
  - Linux
  - WordPress
  - 웹서버
toc: "true"
toc_sticky: "true"
---
### **[Learn] Python 기초와 데이터 처리: 데이터 엔지니어의 필수 도구**

---

### **서론: 데이터 엔지니어에게 Python이 필요한 이유**

현대 데이터 환경에서 데이터 엔지니어의 역할은 매우 중요하다. 대량의 데이터를 수집하고, 정제하며, 효율적인 파이프라인을 구축하는 것이 주요 업무이다. 이러한 과정에서 Python은 핵심적인 프로그래밍 언어로 활용된다.

Python은 간결한 문법과 방대한 라이브러리 생태계를 바탕으로 데이터 처리 작업에 최적화되어 있다. 특히 Pandas와 NumPy는 데이터 엔지니어가 매일 마주하는 다양한 데이터 처리 문제를 해결하는 데 필수적인 도구이다. 본 포스팅은 데이터 엔지니어의 관점에서 Python의 기본적인 활용법과 Pandas, NumPy를 이용한 데이터 처리 기법을 설명한다. 이는 향후 더 복잡한 클라우드 기반 데이터 파이프라인 구축의 단단한 기반이 될 것이다.

---

### **1. 데이터를 다루는 기초 체력: Python 기본 문법**

데이터 엔지니어링은 결국 데이터를 코드로 조작하는 일이다. Python의 기본 문법은 이러한 조작의 가장 기본적인 규칙이 된다.

#### **1.1 Python의 핵심 자료구조와 활용**

데이터를 효율적으로 저장하고 접근하는 것은 데이터 처리의 시작이다.

- **리스트(List)**: 순서가 있는 여러 데이터를 담는 가장 기본적인 자료구조이다. 로그 데이터나 이벤트 스트림처럼 순서가 중요한 데이터를 임시로 저장하고 처리할 때 유용하다.
    
    Python
    
    ```
    # 예시: 리스트 생성 및 접근
    data_list = [10, 20, 30, 40, 50]
    first_element = data_list[0] # 데이터의 첫 번째 요소 접근
    ```
    
- **딕셔너리(Dictionary)**: '키(key)'와 '값(value)'의 쌍으로 데이터를 저장하는 자료구조이다. 특정 속성(키)으로 데이터를 빠르게 찾거나, 구조화된 데이터를 표현할 때 주로 사용된다. API 응답으로 받은 JSON 데이터 처리 시 핵심적으로 활용한다.
    
    Python
    
    ```
    # 예시: 딕셔너리 생성 및 접근
    data_dict = {"name": "Alice", "age": 30, "city": "Seoul"}
    user_name = data_dict["name"] # 'name' 키로 값 접근
    ```
    
- **튜플(Tuple)과 집합(Set)**: 변경 불가능한 순서 있는 데이터(튜플)와 중복을 허용하지 않는 데이터(집합)는 데이터의 무결성을 유지하거나 특정 조건의 데이터를 빠르게 확인하는 데 활용한다.
    

#### **1.2 흐름 제어: 데이터 처리 자동화의 핵심**

데이터는 항상 예상한 대로 흐르지 않는다. 특정 조건에 따라 다른 작업을 수행하거나, 반복적인 작업을 자동화하는 것이 필수적이다.

- **조건문 (if/elif/else)**: 데이터의 특정 값이 특정 조건을 만족할 때만 다른 처리를 해야 할 때 사용한다. 예를 들어, '오류 코드가 400번대일 경우에만 재시도 로직을 실행'하는 조건 처리이다.
    
- **반복문 (for/while)**: 대량의 파일들을 순회하며 처리하거나, 데이터 레코드마다 동일한 작업을 반복해야 할 때 사용한다. 이는 데이터 파이프라인의 자동화를 위한 기본 메커니즘이다.
    
    Python
    
    ```
    # 예시: 간단한 조건문과 반복문
    data_points = [10, 5, 20, 15, 8]
    processed_data = []
    for dp in data_points:
        if dp > 10:
            processed_data.append(dp * 2) # 10보다 크면 두 배로 처리
        else:
            processed_data.append(dp)
    # 결과: [10, 5, 40, 15, 8]
    ```
    

#### **1.3 함수: 재사용 가능한 코드의 단위**

데이터 파이프라인의 각 단계는 독립적인 기능을 수행하는 작은 단위로 구성되는 것이 좋다. 함수는 이러한 독립적인 코드 블록을 만들고 재사용성을 높이는 데 활용한다. 복잡한 전처리 로직을 함수로 만들면 코드의 가독성이 높아지고 유지보수가 쉬워진다.

Python

```
# 예시: 데이터 정제 함수
def clean_string_data(text):
    if text is None:
        return ""
    return text.strip().lower()

# 함수를 활용한 데이터 처리
raw_names = [" Alice ", " Bob ", None, "CATHY"]
cleaned_names = [clean_string_data(name) for name in raw_names]
# 결과: ['alice', 'bob', '', 'cathy']
```

**[여기에 Python 기본 문법을 활용한 데이터 처리 예시 코드 스크린샷을 추가할 수 있다.]**

---

### **2. Pandas와 NumPy는 데이터 처리의 핵심 무기**

Python의 기본 자료구조가 데이터의 '점'을 다룬다면, Pandas와 NumPy는 데이터의 '표'와 '배열'을 다루는 데 최적화되어 있다. 이들은 데이터 엔지니어가 매일 사용하는 가장 중요한 라이브러리이다.

#### **2.1 Pandas: 표 형태 데이터의 마법사 DataFrame**

Pandas는 데이터 처리와 분석을 위한 라이브러리로, 특히 `DataFrame`이라는 강력한 자료구조를 제공한다. DataFrame은 관계형 데이터베이스의 테이블이나 스프레드시트와 유사하게 행(row)과 열(column)로 구성된 2차원 데이터를 표현한다. 이는 다양한 형태의 원천 데이터를 정돈된 형태로 통합하고 전처리하는 데 필수적이다.

- **DataFrame 생성 및 기본 조작**: CSV, Excel, 데이터베이스 등 다양한 소스에서 데이터를 DataFrame으로 로드하고, 컬럼 선택, 행 필터링 등 기본 조작을 수행한다.
    
- **결측치(Missing Values) 처리**: 실제 데이터는 결측치가 흔하다. `fillna()`를 사용해 특정 값으로 채우거나, `dropna()`로 결측치가 있는 행/열을 제거하는 등 데이터의 품질을 관리한다.
    
- **데이터 병합 및 결합**: 여러 파일이나 테이블에 흩어진 데이터를 `merge()`나 `concat()` 함수를 사용하여 논리적으로 연결하고 통합한다. 이는 데이터 파이프라인에서 서로 다른 소스의 데이터를 합치는 핵심 작업이다.
    
- **그룹화 및 집계**: `groupby()`를 통해 특정 기준으로 데이터를 묶고, `sum()`, `mean()`, `count()` 등의 집계 함수를 적용하여 요약된 통계 정보를 산출한다.
    
- **데이터 타입 변환**: `astype()` 등을 사용하여 컬럼의 데이터 타입을 목적에 맞게 변환한다. 데이터가 올바른 타입으로 처리되도록 보장하는 것은 데이터 엔지니어의 중요한 역할이다.
    

**[여기에 Pandas를 활용한 결측치 처리, 데이터 병합, 그룹핑 중 한 가지 이상의 코드 예시와 실행 결과 스크린샷을 추가해야 한다.]**

#### **2.2 NumPy: 고성능 수치 연산의 기반**

NumPy는 고성능 수치 계산을 위한 Python 라이브러리이다. 특히 `ndarray`라는 다차원 배열 객체를 제공하는데, 이는 Pandas의 내부에서도 활용될 만큼 효율적이다. 대규모 데이터셋에 대한 복잡한 수학적 연산을 빠르게 수행해야 할 때 NumPy가 활용된다. 예를 들어, 데이터 정규화나 통계 계산 시 사용된다.

Python

```
# 예시: NumPy를 활용한 배열 연산
import numpy as np

data_array = np.array([1, 2, 3, 4, 5])
processed_array = data_array * 10 + 5 # 모든 요소에 대한 고속 연산
# 결과: [15, 25, 35, 45, 55]

# Pandas DataFrame과 NumPy 연동 예시
import pandas as pd
df_np_example = pd.DataFrame({'value': [10, 20, 30]})
df_np_example['squared'] = np.power(df_np_example['value'], 2)
# 결과:
#    value  squared
# 0     10      100
# 1     20      400
# 2     30      900
```

**[여기에 NumPy 활용 예시와 Pandas DataFrame에 NumPy 연산을 적용하는 코드 스크린샷을 추가할 수 있다.]**

---

### **3. 성능과 효율에 대한 고려: 벡터화 연산의 중요성**

Python으로 데이터를 처리할 때, 단순히 코드를 작성하는 것을 넘어 **성능과 효율**을 고려하는 것은 데이터 엔지니어에게 필수적인 역량이다. Pandas와 NumPy가 강력한 이유 중 하나는 바로 **벡터화 연산(Vectorized Operations)** 때문이다.

일반적인 Python `for` 루프는 데이터를 하나씩 처리하므로 대규모 데이터에서는 매우 느려진다. 반면, Pandas나 NumPy는 내부적으로 C나 Fortran과 같은 최적화된 저수준 언어로 구현되어 있어, 데이터 배열 전체에 대한 연산을 한 번에 수행할 수 있다. 이는 데이터 처리 속도를 혁신적으로 향상시킨다. `for` 루프 대신 Pandas나 NumPy의 내장 함수를 활용하는 습관은 효율적인 데이터 파이프라인을 구축하는 데 필수적이다.

---

### **4. 결론: 다음 단계로의 연결고리**

이 포스팅에서 다룬 Python의 기초 문법과 Pandas, NumPy를 활용한 데이터 처리 기법은 모든 데이터 엔지니어링 작업의 가장 단단한 토대가 된다. 실제 대규모 데이터 환경에서는 Pandas의 한계를 넘어서는 PySpark와 같은 분산 처리 프레임워크가 필요하지만, 그 기반에는 항상 Python으로 데이터를 이해하고 조작하는 능력이 깔려 있다.

군주는 이 기본기를 바탕으로, 향후 PySpark와 Databricks를 활용한 더욱 복잡한 데이터 파이프라인을 구축하는 다음 단계로 나아갈 준비를 마쳤다. 이 포스팅이 군주의 데이터 엔지니어링 여정에서 중요한 이정표가 되기를 바란다.